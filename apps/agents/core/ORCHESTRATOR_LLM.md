# LLM-Based Orchestrator - Intelligente Routing-Entscheidungen

## üéØ √úbersicht

Der **LLMOrchestratorAgent** nutzt ein LLM (GPT-5.2 oder neuestes Modell) f√ºr intelligente Routing-Entscheidungen basierend auf OpenAI Agent SDK Patterns.

## üöÄ Warum LLM f√ºr Orchestrator?

### Vorteile
1. **Intelligente Entscheidungen**: LLM versteht Kontext und Nuancen
2. **Adaptiv**: Passt sich an neue Situationen an
3. **Kontext-Aware**: Nutzt Konversations-Historie
4. **Reasoning**: Kann komplexe Entscheidungen begr√ºnden

### Vergleich

| Aspekt | Logik-basiert | LLM-basiert |
|--------|---------------|-------------|
| **Intelligenz** | ‚ùå Regel-basiert | ‚úÖ Kontext-verstehend |
| **Adaptivit√§t** | ‚ùå Statisch | ‚úÖ Dynamisch |
| **Komplexit√§t** | ‚ùå Begrenzt | ‚úÖ Hoch |
| **Geschwindigkeit** | ‚úÖ Sehr schnell | ‚ö†Ô∏è Etwas langsamer (~300ms) |

## üîß Modell-Konfiguration

### Neuestes Modell (Standard)

```bash
# GPT-5.2 (Dezember 2025) - Neuestes Modell
ORCHESTRATOR_MODEL=gpt-5.2
```

### Verf√ºgbare Modelle (Fallback-Kette)

1. **gpt-5.2** (Dezember 2025) - Neuestes, beste Performance
2. **gpt-5.1** (November 2025) - Sehr gut
3. **gpt-5** (August 2025) - Gut
4. **gpt-4.1** (April 2025) - Fallback
5. **gpt-4o** (May 2024) - Letzter Fallback

### Konfiguration

```bash
# In .env Datei
ORCHESTRATOR_MODEL=gpt-5.2

# LLM API Key
OPENAI_API_KEY=sk-...
# Oder via LiteLLM Gateway
LITELLM_MASTER_KEY=...
```

## üìã OpenAI Agent SDK Pattern

### LLM-Driven Orchestration

Der Orchestrator nutzt **LLM-Driven Orchestration** Pattern:

1. **System Prompt**: Definiert verf√ºgbare Agents und Routing-Regeln
2. **Tools**: Agents als Tools f√ºr LLM (route_to_agent, escalate_to_human)
3. **Tool Choice**: LLM w√§hlt autonom passenden Agent
4. **Reasoning**: LLM begr√ºndet Entscheidung

### Tool-basierte Agent-Auswahl

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "route_to_agent",
            "description": "Routet zu passendem Agent",
            "parameters": {
                "agent_name": "...",
                "reasoning": "...",
                "confidence": 0.0-1.0
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "escalate_to_human",
            "description": "Eskaliert zu Human",
            ...
        }
    }
]
```

## üéØ Verwendung

### Standard (nutzt ENV)

```python
from apps.agents.core.llm_orchestrator_agent import get_llm_orchestrator, RoutingRequest

orchestrator = get_llm_orchestrator()  # Nutzt ORCHESTRATOR_MODEL aus ENV

request = RoutingRequest(
    account_id="restaurant-123",
    user_message="Ich m√∂chte einen Tisch f√ºr morgen Abend reservieren, aber ich habe eine Allergie",
    channel="phone",
    conversation_history=[
        {"role": "user", "content": "Hallo"},
        {"role": "assistant", "content": "Guten Tag, wie kann ich helfen?"}
    ]
)

response = orchestrator.route(request)

print(f"Agent: {response.target_agent}")
print(f"Reasoning: {response.reasoning}")
print(f"Confidence: {response.confidence}")
```

### Mit Custom LLM Client

```python
import litellm
from apps.agents.core.llm_orchestrator_agent import LLMOrchestratorAgent

orchestrator = LLMOrchestratorAgent(llm_client=litellm)
```

## üß† Intelligente Features

### 1. Kontext-Verst√§ndnis

```python
# LLM versteht komplexe Anfragen
request = RoutingRequest(
    user_message="Ich war letzte Woche da und hatte ein Problem mit meiner Bestellung. Jetzt m√∂chte ich wieder bestellen, aber diesmal sicherstellen dass es richtig ist."
)

# LLM erkennt:
# - Beschwerde-Kontext (letzte Woche)
# - Aktueller Intent (Bestellung)
# - Routing: Erst Beschwerde l√∂sen, dann Bestellung
```

### 2. Konversations-Historie

```python
request = RoutingRequest(
    user_message="Ja, das passt",
    conversation_history=[
        {"role": "assistant", "content": "M√∂chten Sie einen Tisch f√ºr 4 Personen am Samstag um 19 Uhr?"},
        {"role": "user", "content": "Ja, das passt"}
    ]
)

# LLM versteht Kontext aus Historie
```

### 3. Multi-Intent Erkennung

```python
# Komplexe Anfrage mit mehreren Intents
request = RoutingRequest(
    user_message="Ich m√∂chte einen Tisch reservieren und habe eine Frage zu Allergenen im Men√º"
)

# LLM kann entscheiden:
# - Prim√§r: Reservierung ‚Üí restaurant_voice_host_agent
# - Sekund√§r: Allergen-Frage kann im gleichen Agent behandelt werden
```

## üìä Performance

### Mit GPT-5.2

```
LLM Routing-Entscheidung: ~300ms
Agent-Instanziierung: ~10ms
Total: ~310ms
```

### Vergleich mit Logik-basiert

```
Logik-basiert: ~50ms (aber weniger intelligent)
LLM-basiert: ~310ms (aber viel intelligenter)
```

**Trade-off**: Etwas langsamer, aber deutlich intelligenter!

## ‚öôÔ∏è Konfiguration

### Environment Variables

```bash
# Orchestrator Model
ORCHESTRATOR_MODEL=gpt-5.2

# LLM API
OPENAI_API_KEY=sk-...
# Oder
LITELLM_MASTER_KEY=...
GATEWAY_BASE_URL=http://gateway:4000
```

### Code-Level

```python
import os
os.environ["ORCHESTRATOR_MODEL"] = "gpt-5.2"

orchestrator = get_llm_orchestrator()
```

## üîç Monitoring

Routing-Entscheidungen werden automatisch getrackt:

```python
from apps.agents.core.monitoring import get_monitor

monitor = get_monitor()
stats = monitor.get_routing_stats()

# Enth√§lt:
# - Routing-Entscheidungen
# - Confidence-Levels
# - Reasoning (aus LLM)
```

## üêõ Troubleshooting

### Problem: Modell nicht verf√ºgbar

**L√∂sung:**
1. Pr√ºfe Modell-Name (gpt-5.2, gpt-5.1, etc.)
2. Nutze Fallback-Kette (automatisch)
3. Pr√ºfe API Key

### Problem: Zu langsam

**L√∂sung:**
1. Nutze schnelleres Modell (gpt-4o statt gpt-5.2)
2. Reduziere max_tokens
3. Nutze Caching

---

**Version:** 1.0.0  
**Basiert auf:** OpenAI Agent SDK, LLM-Driven Orchestration Pattern
